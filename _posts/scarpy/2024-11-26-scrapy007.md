---
layout: post
title : beautifulsoup 套件使用方法
date : 2024-11-26 11:24:29 +0800
categories: 
    - scrapy
    - python
---

## **1. 安装 Beautiful Soup 和依赖**

安装 `bs4` 和 HTML 解析器 `lxml`：
```bash
pip install bs4 lxml
```

## **2. 基础使用**
### **抓取 HTML 并解析**
```python
from bs4 import BeautifulSoup

# 示例 HTML
html = """
<html>
    <head><title>Beautiful Soup 教学</title></head>
    <body>
        <h1>这是一个标题</h1>
        <p>这是一个段落。</p>
        <a href="https://example.com">这是一个链接</a>
    </body>
</html>
"""

# 使用 lxml 解析 HTML
soup = BeautifulSoup(html, "lxml")

# 打印 HTML 的格式化结构
print(soup.prettify())
```

## **3. 查找元素**
Beautiful Soup 提供多种方式查找元素：

### **查找单个元素**
- **通过标签名**：
```python
title = soup.title
print(title.text)  # Beautiful Soup 教学
```

- **通过属性查找**：
```python
link = soup.find("a")  # 找到第一个 <a> 标签
print(link["href"])    # https://example.com
```

### **查找多个元素**
```python
paragraphs = soup.find_all("p")  # 找到所有 <p> 标签
for p in paragraphs:
    print(p.text)
```

## **4. CSS 选择器查找**
Beautiful Soup 支持使用 CSS 选择器来查找元素：
```python
# 使用 CSS 类选择
paragraph = soup.select_one("p")  # 找到第一个 <p> 标签
print(paragraph.text)

# 使用复杂的选择器
all_links = soup.select("a[href]")  # 找到所有带 href 属性的 <a> 标签
for link in all_links:
    print(link["href"])
```

## **5. 获取和修改内容**
### **获取内容**
```python
h1 = soup.find("h1")
print(h1.text)  # 获取文本内容
```

### **修改内容**
```python
h1.string = "新的标题"
print(soup.h1)  # 修改后的 HTML
```

## **6. 遍历 DOM 树**
### 获取父节点和子节点：
```python
# 父节点
parent = soup.h1.parent
print(parent.name)  # body

# 子节点
body_children = soup.body.contents
print(body_children)  # 列表形式显示 body 的直接子节点
```

### 获取兄弟节点：
```python
# 下一个兄弟节点
next_sibling = soup.h1.next_sibling
print(next_sibling)  # 空白或下一个标签

# 上一个兄弟节点
previous_sibling = soup.a.previous_sibling
print(previous_sibling)  # 可能为空白
```

## **7. 解析网络上的 HTML**
结合 `requests` 抓取网页内容：
```python
import requests
from bs4 import BeautifulSoup

url = "https://example.com"
response = requests.get(url)

# 解析网页内容
soup = BeautifulSoup(response.content, "lxml")

# 提取所有链接
for a_tag in soup.find_all("a", href=True):
    print(a_tag["href"])
```

## **8. 处理动态内容（JavaScript 渲染）**
Beautiful Soup 无法处理动态内容（如 JavaScript 加载的数据）。可以结合 Selenium 抓取动态网页：
```python
from selenium import webdriver
from bs4 import BeautifulSoup

driver = webdriver.Chrome("path/to/chromedriver")
driver.get("https://example.com")

# 获取动态渲染后的 HTML
html = driver.page_source

# 使用 Beautiful Soup 解析
soup = BeautifulSoup(html, "lxml")

driver.quit()
```

## **9. 常见操作**
### 获取属性值
```python
link = soup.find("a")
print(link["href"])  # 获取 href 属性
```

### 查找带特定属性的标签
```python
button = soup.find("button", {"class": "btn-primary"})
print(button.text)
```

### 删除标签
```python
soup.h1.decompose()  # 删除 <h1> 标签
print(soup)
```

## **10. 高级应用**
### **查找正则匹配**
结合 `re` 模块查找内容：
```python
import re

links = soup.find_all("a", href=re.compile(r"https://"))
for link in links:
    print(link["href"])
```

### **提取表格内容**
```python
html = """
<table>
    <tr><th>名称</th><th>价格</th></tr>
    <tr><td>苹果</td><td>5</td></tr>
    <tr><td>香蕉</td><td>3</td></tr>
</table>
"""
soup = BeautifulSoup(html, "lxml")

# 提取表格行
rows = soup.select("table tr")
for row in rows:
    cols = row.find_all("td")
    data = [col.text for col in cols]
    print(data)
```

### **结合 Pandas 保存数据**
可以使用 Pandas 将提取的数据保存为 CSV 文件：
```python
import pandas as pd

data = []
rows = soup.select("table tr")[1:]  # 跳过表头
for row in rows:
    cols = row.find_all("td")
    data.append([col.text for col in cols])

# 保存为 CSV
df = pd.DataFrame(data, columns=["名称", "价格"])
df.to_csv("output.csv", index=False)
```