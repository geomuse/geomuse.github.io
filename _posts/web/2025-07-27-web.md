---
layout: post
title:  python网络爬虫实战-requests
date:   2025-07-27 09:01:00 +0800
tags: 
    - python
    - web
image: 02.jpg
---

大家好，我是python网络爬虫这门课程的主要讲师geo 

在正式学习 requests 之前，我们必须先搞清楚一个问题：

# **爬虫到底在做什么？**

### 人类访问网页的过程

当你在浏览器输入：

```
https://www.google.com
```

浏览器会做几件事情：

1. 向服务器发出 **HTTP 请求**
2. 服务器接收到请求
3. 服务器返回 **HTTP 响应**
4. 浏览器解析 HTML / JSON / 图片
5. 渲染页面给你看

### 爬虫做的事情

爬虫做的事情 **一模一样**，只是：

| 人类    | 爬虫              |
| ----- | --------------- |
| 浏览器   | Python requests |
| 眼睛看网页 | 程序解析数据          |
| 手动复制  | 自动保存            |

**requests 就是“程序版浏览器”**

---

# 什么是requests

Python 中一个 非常简单易用的 HTTP 请求库，主要用来与网络上的服务器进行沟通

## requests 的官方定义

> Requests is a simple, yet elegant HTTP library.

中文解释：

> **requests 是 Python 中一个非常简单、非常优雅的 HTTP 请求库**

## 为什么爬虫一定要学 requests？

因为：

* 它是 **Python 爬虫的第一步**
* 几乎 **所有爬虫框架（Scrapy / Playwright / Selenium）底层都离不开 HTTP**
* 所有 API 抓取、本地测试、量化交易接口，**100% 都用到 requests 思维**

---

# HTTP 协议基础（爬虫必懂）

## 什么是 HTTP？

HTTP（HyperText Transfer Protocol）是：

> 客户端（Client）与服务器（Server）之间的通信规则

## 请求与响应模型

```text
Client (你 / 爬虫)
   ↓ 请求（Request）
Server
   ↑ 响应（Response）
```

---

## 四、HTTP 请求的四大组成部分

| 部分      | 说明                              |
| ------- | ------------------------------- |
| URL     | 请求地址                            |
| Method  | 请求方法（GET / POST / PUT / DELETE） |
| Headers | 请求头（伪装浏览器）                      |
| Body    | 请求数据（表单 / JSON）                 |

---

# requests 的使用方法

```bash
pip install requests
```

可用于发送各种类型的 HTTP 请求，如 GET、POST、PUT、DELETE 等

## 发起 GET 请求

该网站会判断客户端是否为GET请求，如果是，那么它将会返回相对应的请求信息

```py
import requests

response = requests.get('https://httpbin.org/get')
print(response.status_code)      # 状态码
print(response.text)             # 返回的 HTML 内容
```

### response 是什么？

`response` 是一个 **Response 对象**

它包含了：

| 属性                   | 说明         |
| -------------------- | ---------- |
| response.status_code | HTTP 状态码   |
| response.text        | 字符串形式的返回内容 |
| response.content     | bytes（二进制） |
| response.json()      | JSON 数据    |
| response.headers     | 响应头        |

---

# 状态码 status code

这是通过server端会回传的状态码

为什么状态码这么重要？

爬虫本质是：

> **判断网页是否成功 → 决定要不要继续处理**

服务器对客户端请求的响应结果，用来说明服务器是否成功处理了请求，或者发生了什么错误。

| 状态码   | 含义               | 示例说明               |
| ----- | ---------------- | ------------------ |
| `200` | OK，请求成功          | 最常见。成功获取网页、API 数据等 |
| `201` | Created，资源已创建    | 如 POST 创建资源成功      |
| `204` | No Content，无内容返回 | 通常用于 DELETE 操作     |
| `301` | 永久重定向            | 网址永久搬家     |
| `302` | 临时重定向            | 网址临时跳转     |
| `304` | Not Modified，未修改 | 浏览器缓存使用的响应 |
| `400` | Bad Request，请求错误       | 参数格式不对、缺字段   |
| `401` | Unauthorized，未授权       | 需要登录或 Token  |
| `403` | Forbidden，被禁止访问        | 有权限问题        |
| `404` | Not Found，资源未找到        | URL 错误或数据不存在 |
| `429` | Too Many Requests，过载请求 | 爬虫被封、限流触发    |
| `500` | Internal Server Error，服务器内部错误 | 程序崩溃或bug |
| `502` | Bad Gateway，网关错误              | 服务中间层出问题 |
| `503` | Service Unavailable，服务不可用     | 服务器过载或维护 |
| `504` | Gateway Timeout，网关超时          | 响应超时     |

<!-- 对url进行get请求 -->

---

# 状态码判断示例

```python
if response.status_code == 200:
    print("请求成功")
else:
    print("请求失败", response.status_code)
```

---

# 带参数的 GET 请求（params）

## URL 查询参数

```text
https://example.com/search?q=python&page=2
```

## 使用 params

```python
params = {
    'q': 'python',
    'page': 2
}

response = requests.get(url, params=params)
```

requests 会自动拼接 URL。

---

# 发起 POST 请求（提交表单）

- 登录
- 注册
- 提交表单
- 创建资源

```py
payload = {'username': 'test', 'password': '123456'}
response = requests.post('https://httpbin.org/post', data=payload)
print(response.json())
```

# 发送 `PUT` 请求通常用于**更新资源**。例如：更新数据库中某条记录的内容。

```python
import requests

url = 'https://httpbin.org/put'
data = {
    'id': '123',
    'name': 'Boon Hong'
}

response = requests.put(url, data=data)
print(response.status_code)
print(response.text)
```

# 发送 JSON 数据

```python
json_data = {
    'id': '123',
    'status': 'active'
}

response = requests.put(url, json=json_data)
print(response.json())
```

| 类型   | 使用场景   |
| ---- | ------ |
| data | 表单提交   |
| json | API 请求 |
    
# `DELETE`

发送 `DELETE` 请求用于**删除资源**，比如删除一个用户、文章、数据等。

```python
url = 'https://httpbin.org/delete'
params = {'id': '123'}

response = requests.delete(url, params=params)
print(response.status_code)
print(response.json())
```

<!-- # 通过`get`爬取网页

`re` 为简单快速学习，正则表达式用chatgpt处理就好，我们只需要搞懂基本的web爬取流程即可。

```py
import requests , re

r = requests.get('https://ssr1.scrape.center/',verify=False)
exit() if not r.status_code == requests.codes.ok else print('ok')
pattern = re.compile('<h2.*?>(.*?)</h2>',re.S)
title = re.findall(pattern,r.text)
print(title)
``` -->

# 超时与异常处理

```python
try:
    response = requests.get(url, timeout=5)
except requests.exceptions.RequestException as e:
    print(e)
```

# 上传文件

```python
files = {'file': open('test.txt', 'rb')}
response = requests.post('https://httpbin.org/post', files=files)
```

# 文件下载

```python
response = requests.get(img_url)

with open('img.jpg', 'wb') as f:
    f.write(response.content)
```