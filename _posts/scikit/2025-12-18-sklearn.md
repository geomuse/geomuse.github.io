---
layout: post
title:  python机器学习 模型优化与超参数调优 (Model Optimization and Hyperparameter Tuning)
date:   2025-12-18 09:01:00 +0800
image: 11.jpg
tags: 
    - python
    - sklearn
---

成本函数（Cost Function），也称为损失函数（Loss Function）或误差函数（Error Function），在机器学习和深度学习中是至关重要的，因为它衡量了模型的预测结果与实际目标值（真实值）之间的差异，训练算法的目标就是最小化此函数的值

优化旨在找到最佳模型参数（通过训练）和最佳超参数（通过调整），同时应用正则化技术以管理偏差和方差。

**1. 超参数调优方法**

*   **网格搜索 (Grid Search)**：系统地评估所有超参数组合，通常用于超参数空间较小的场景。
*   **随机搜索 (Randomized Search)**：在超参数空间中随机抽样，当搜索空间较大时，通常比网格搜索更有效率。

**2. 正则化技术 (Regularization)**

正则化通过约束模型来降低过拟合的风险，是优化过程的核心部分。

| 正则化方法 | 目标/机制 | 适用模型 | 来源引用 |
| :--- | :--- | :--- | :--- |
| **L2正则化 (Ridge Regression)** | **约束模型权重**，将权重的L2范数添加到成本函数，使权重尽可能小。 | 线性模型、DNN | |
| **L1正则化 (Lasso Regression)** | **倾向于消除不重要特征的权重**（将其设为零），导致稀疏模型。适用于你怀疑只有少数特征重要的场景。 | 线性模型、DNN | |
| **弹性网络 (Elastic Net)** | L1和L2正则项的简单混合，通常比单独使用Lasso更稳定。 | 线性模型、DNN | |
| **提前停止 (Early Stopping)** | **迭代学习算法**（如梯度下降）的正则化方法，在验证误差达到最小值时停止训练。 | DNN、迭代算法 | |
| **Dropout** | 在每个训练步骤中随机“删除”神经元，**迫使网络分散学习**，对输入变化不敏感，是深度学习中最受欢迎的正则化技术之一,。 | DNN |, |

**3. 优化算法 (Optimizers)**

梯度下降（GD）是许多模型最常用的训练方法，用于最小化成本函数。

*   **GD变体**：包括批量梯度下降（Batch GD）、随机梯度下降（SGD）和**小批量梯度下降**（Mini-batch GD）。SGD和Mini-batch GD可以处理大型数据集，是核外学习的理想选择,,。
*   **高级优化器**：如动量优化（Momentum）、RMSProp、AdaGrad、**Adam**和**Nadam**，它们通过自适应学习率和动量来加速收敛，尤其适用于训练深度神经网络（DNN）,,,。
*   **学习率调度 (Learning Rate Scheduling)**：通过逐渐降低学习率（例如，使用指数衰减或1周期调度）来确保算法不会跳过最优解并加速收敛,,。

