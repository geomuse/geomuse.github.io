---
layout: post
title:  python机器学习 关联规则与异常检测
date:   2025-12-25 09:01:00 +0800
image: 11.jpg
tags: 
    - python
    - scikit
---

## 关联规则分析（Association Rule Mining）

### 什么是关联规则

关联规则的核心问题只有一句话：

> 哪些东西「经常一起出现」？

最经典的例子是超市购物篮分析：

* 买了面包的人，是否也常买牛奶？
* 买了尿布的人，是否常买啤酒？

在数据中，这类问题被抽象为：

* 一笔交易（Transaction）＝ 一个样本
* 商品集合（Itemset）＝ 一组同时出现的特征

---

### 关联规则的基本形式

关联规则通常写成：

```
A → B
```

含义是：

> 如果出现 A，那么很可能也会出现 B

例如：

```
{牛奶, 面包} → {鸡蛋}
```

---

### 三个核心指标

#### 支持度（Support）

表示某个项目集在所有交易中出现的比例。

```
Support(A) = 出现 A 的交易数 / 总交易数
```

直觉理解：

* 出现得够不够「频繁」
* 过滤掉极少发生的组合

---

#### 置信度（Confidence）

表示在出现 A 的情况下，也出现 B 的概率。

```
Confidence(A → B) = Support(A ∪ B) / Support(A)
```

直觉理解：

* A 出现后，B 跟不跟得上
* 是「条件概率」

---

#### 提升度（Lift）

衡量 A 和 B 是否真的有关联，而不是巧合。

```
Lift(A → B) = Confidence(A → B) / Support(B)
```

解释：

* Lift > 1：正相关
* Lift = 1：独立
* Lift < 1：负相关

在实际应用中，**Lift 比 Confidence 更重要**。

---

## Apriori 算法

### Apriori 的核心思想

Apriori 的思想非常简单：

> 如果一个组合是频繁的，那么它的所有子集也一定是频繁的

反过来说：

> 如果一个组合是不频繁的，那么包含它的更大组合一定不频繁

这被称为 **Apriori 剪枝原则**。

---

### Apriori 的运作流程

1. 找出所有「频繁 1 项集」
2. 由频繁 1 项集组合成候选 2 项集
3. 计算支持度，过滤不够频繁的
4. 重复扩展到 3 项集、4 项集
5. 由频繁项集生成关联规则

---

### Apriori 的缺点

* 需要反复扫描数据库
* 当特征很多时，候选集爆炸
* 计算效率较低

这也是 FP-Growth 出现的原因。

---

## FP-Growth 算法

### FP-Growth 的核心改进

FP-Growth 不再「生成候选集」。

它的思路是：

* 把数据压缩成一棵 **FP-Tree**
* 在树结构中直接挖掘频繁项集

---

### FP-Growth 的优势

* 速度快
* 适合高维稀疏数据
* 不需要多次扫描数据库

---

### Apriori vs FP-Growth 对比

| 项目      | Apriori | FP-Growth |
| ------- | ------- | --------- |
| 是否生成候选集 | 是       | 否         |
| 数据扫描次数  | 多       | 少         |
| 计算效率    | 较慢      | 较快        |
| 实务使用    | 教学为主    | 工业常用      |

---

## 关联规则的实际应用场景

* 电商推荐系统（搭配购买）
* 超市陈列优化
* 内容推荐
* 医疗共病分析
* 网页点击路径分析

---

## 异常检测（Anomaly Detection）

---

### 什么是异常检测

异常检测要解决的问题是：

> 找出「和大多数数据不一样」的样本

注意重点：

* **不一定有标签**
* 异常往往是少数
* 分布通常不规则

---

### 异常检测的常见应用

* 信用卡盗刷
* 洗钱行为
* 网络入侵检测
* 设备故障预测
* 欺诈交易识别

---

## Isolation Forest（隔离森林）

### Isolation Forest 的核心思想

传统模型先学习「正常行为」，再找异常。

Isolation Forest 反过来：

> 异常点更容易被「隔离」

---

### 什么叫「隔离」

算法做的事情：

1. 随机选特征
2. 随机选切分点
3. 不断切分数据

异常点通常：

* 数量少
* 特征值极端
* 很快被单独切出来

---

### Isolation Forest 的优点

* 不需要假设分布
* 适合高维数据
* 计算速度快
* 工业界非常常用

---

### sklearn 使用直觉

```python
from sklearn.ensemble import IsolationForest

model = IsolationForest(
    n_estimators=100,
    contamination=0.05,
    random_state=42
)

model.fit(X)
y_pred = model.predict(X)
```

输出结果：

* `1`：正常
* `-1`：异常

---

## One-Class SVM

### One-Class SVM 的基本想法

One-Class SVM 的目标是：

> 学习一个「包住大多数正常点」的边界

在边界外的点，被认为是异常。

---

### 与传统 SVM 的差异

| 项目       | 二分类 SVM | One-Class SVM |
| -------- | ------- | ------------- |
| 是否需要异常标签 | 需要      | 不需要           |
| 学习目标     | 分开两类    | 描述正常类         |
| 应用       | 分类      | 异常检测          |

---

### One-Class SVM 的特点

优点：

* 理论基础扎实
* 能建复杂非线性边界

缺点：

* 对参数敏感
* 对数据规模不友好
* 高维时速度慢

---

### sklearn 使用示意

```python
from sklearn.svm import OneClassSVM

model = OneClassSVM(
    kernel='rbf',
    nu=0.05,
    gamma='scale'
)

model.fit(X)
y_pred = model.predict(X)
```

---

## Isolation Forest vs One-Class SVM

| 项目    | Isolation Forest | One-Class SVM |
| ----- | ---------------- | ------------- |
| 数据规模  | 大                | 小中            |
| 高维表现  | 好                | 较差            |
| 参数敏感度 | 低                | 高             |
| 实务使用  | 非常常见             | 较少            |

**实务建议**：
如果你不确定用什么，先用 Isolation Forest。

---

## 本阶段学习重点总结

你在 Day 47–50 应该掌握的是：

* 关联规则不是预测模型，而是「模式挖掘」
* Support、Confidence、Lift 的实际意义
* Apriori 与 FP-Growth 的效率差异
* 异常检测多数是无监督问题
* Isolation Forest 的直觉非常重要
* One-Class SVM 适合理解，不一定是首选
